{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shared.data_utils as data_utils\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import modules\n",
    "#import modules2 as modules\n",
    "import time\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "device_type = 'cuda'\n",
    "device = torch.device(device_type)\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "batch_size = 4\n",
    "shuffle = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshDataset(Dataset):\n",
    "    def __init__(self, mesh_list):\n",
    "        self.mesh_list = mesh_list \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mesh_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.mesh_list[idx]\n",
    "    \n",
    "    def map(self, func):\n",
    "        for i in range(len(self.mesh_list)):\n",
    "           self.mesh_list[i] = func(self.mesh_list[i])\n",
    "        return self\n",
    "\n",
    "# Prepare synthetic dataset\n",
    "ex_list = []\n",
    "for k, mesh in enumerate(['cube', 'cylinder', 'cone', 'icosphere']):\n",
    "    mesh_dict = data_utils.load_process_mesh(\n",
    "        os.path.join('meshes', '{}.obj'.format(mesh)))\n",
    "    mesh_dict['class_label'] = torch.tensor(k)\n",
    "    mesh_dict['vertices'] = torch.tensor(mesh_dict['vertices'])\n",
    "    mesh_dict['faces'] = torch.tensor(mesh_dict['faces'])\n",
    "    ex_list.append(mesh_dict)\n",
    "\n",
    "synthetic_dataset = MeshDataset(ex_list)\n",
    "\n",
    "# Plot the meshes\n",
    "mesh_list = []\n",
    "\n",
    "for shape in synthetic_dataset:\n",
    "    mesh_list.append(\n",
    "      {'vertices': data_utils.dequantize_verts(torch.tensor(shape['vertices'])),\n",
    "        'faces': data_utils.unflatten_faces(torch.tensor(shape['faces']))})\n",
    "data_utils.plot_meshes(mesh_list, ax_lims=0.4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vertex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for vertex model training\n",
    "def pad_batch(batch):\n",
    "    # group matching keys in batch\n",
    "    items = list(zip(*[item.values() for item in batch]))\n",
    "    packed_dict = {}\n",
    "    for i, key in enumerate(batch[0].keys()):\n",
    "        if items[i][0].dim() == 0:\n",
    "            padded_values = torch.tensor(items[i], device=device)\n",
    "        else:\n",
    "            padded_values = torch.nn.utils.rnn.pad_sequence(items[i], batch_first=True, padding_value=0.).to(device)\n",
    "        packed_dict[key] = padded_values\n",
    "    return packed_dict\n",
    "\n",
    "vertex_model_dataset = data_utils.make_vertex_model_dataset(\n",
    "    synthetic_dataset, apply_random_shift=False)\n",
    "\n",
    "\n",
    "vertex_model_dataloader = iter(DataLoader(vertex_model_dataset, \n",
    "                                          shuffle=shuffle, \n",
    "                                          batch_size=batch_size, \n",
    "                                          collate_fn=pad_batch))\n",
    "vertex_model_batch = next(vertex_model_dataloader)\n",
    "\n",
    "max_num_input_verts=250\n",
    "\n",
    "decoder_config={\n",
    "        'embd_size': 128,\n",
    "        'fc_size': 512, \n",
    "        'num_layers': 3,\n",
    "        'dropout_rate': 0.,\n",
    "        'take_context_embedding': False\n",
    "}\n",
    "\n",
    "# Create vertex model\n",
    "vertex_model = modules.VertexModel(\n",
    "    decoder_config=decoder_config,\n",
    "    class_conditional=True,\n",
    "    context_type='label',\n",
    "    num_classes=4,\n",
    "    max_num_input_verts=max_num_input_verts,\n",
    "    quantization_bits=8,\n",
    "    device=device\n",
    ").to(device=device)\n",
    "\n",
    "with ctx:\n",
    "    vertex_model_pred_dist = vertex_model(vertex_model_batch)\n",
    "\n",
    "vertex_model_loss = -torch.sum(\n",
    "    vertex_model_pred_dist.log_prob(vertex_model_batch['vertices_flat']) * \n",
    "    vertex_model_batch['vertices_flat_mask'])\n",
    "\n",
    "with ctx:\n",
    "    vertex_samples = vertex_model.sample(\n",
    "    4, context=vertex_model_batch, max_sample_length=max_num_input_verts, top_p=0.95,\n",
    "    recenter_verts=False, only_return_complete=False)\n",
    "\n",
    "print(vertex_model_batch)\n",
    "print(vertex_model_pred_dist)\n",
    "print(vertex_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model_dataset = data_utils.make_face_model_dataset(\n",
    "    synthetic_dataset, apply_random_shift=False)\n",
    "\n",
    "face_model_dataloader = iter(DataLoader(face_model_dataset, \n",
    "                                        shuffle=shuffle, \n",
    "                                        batch_size=batch_size,\n",
    "                                        collate_fn=pad_batch))\n",
    "face_model_batch = next(face_model_dataloader)\n",
    "\n",
    "encoder_config={\n",
    "        'embd_size': 128,\n",
    "        'fc_size': 512, \n",
    "        'num_layers': 3,\n",
    "        'num_heads': 4,\n",
    "        'dropout_rate': 0.\n",
    "}\n",
    "decoder_config={\n",
    "    'embd_size': 128,\n",
    "    'fc_size': 512, \n",
    "    'num_layers': 3,\n",
    "    'dropout_rate': 0.,\n",
    "    'num_heads': 4,\n",
    "    'take_context_embedding': True\n",
    "}\n",
    "\n",
    "# Create face model\n",
    "face_model = modules.FaceModel(\n",
    "    encoder_config=encoder_config,\n",
    "    decoder_config=decoder_config,\n",
    "    class_conditional=False,\n",
    "    max_seq_length=1000,\n",
    "    quantization_bits=8,\n",
    "    max_num_input_verts=max_num_input_verts,\n",
    "    decoder_cross_attention=True,\n",
    "    use_discrete_vertex_embeddings=True,\n",
    "    device=device\n",
    ").to(device=device)\n",
    "\n",
    "with ctx:\n",
    "    face_model_pred_dist = face_model(face_model_batch)\n",
    "face_model_loss = -torch.sum(face_model_pred_dist.log_prob(face_model_batch['faces']) * \n",
    "    face_model_batch['faces_mask'])\n",
    "\n",
    "with ctx:\n",
    "    face_samples = face_model.sample(\n",
    "    context=vertex_samples, max_sample_length=500, top_p=0.95,\n",
    "    only_return_complete=False)\n",
    "print(face_model_batch)\n",
    "print(face_model_pred_dist)\n",
    "print(face_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization settings\n",
    "learning_rate = 8e-4 #3e-4\n",
    "training_steps = 300\n",
    "log_step = 5\n",
    "sample_step = 100\n",
    "n_samples = 4\n",
    "\n",
    "# Create an optimizer an minimize the summed log probability of the mesh sequences\n",
    "face_model_optim = torch.optim.AdamW(face_model.parameters(), lr=learning_rate)\n",
    "vertex_model_optim = torch.optim.AdamW(vertex_model.parameters(), lr=learning_rate)\n",
    "\n",
    "vertex_model_dataloader = DataLoader(vertex_model_dataset, \n",
    "                                     shuffle=shuffle, \n",
    "                                     batch_size=batch_size, \n",
    "                                     collate_fn=pad_batch)\n",
    "face_model_dataloader = DataLoader(face_model_dataset, \n",
    "                                   shuffle=shuffle, \n",
    "                                   batch_size=batch_size, \n",
    "                                   collate_fn=pad_batch)\n",
    "vertex_model_dataloader_iter = iter(vertex_model_dataloader)\n",
    "face_model_dataloader_iter = iter(face_model_dataloader)\n",
    "\n",
    "# Training loop\n",
    "for n in range(training_steps):\n",
    "    try:\n",
    "      vertex_model_batch = next(vertex_model_dataloader_iter)\n",
    "    except StopIteration:\n",
    "      vertex_model_dataloader_iter = iter(vertex_model_dataloader)\n",
    "      vertex_model_batch = next(vertex_model_dataloader_iter)\n",
    "\n",
    "    try:\n",
    "      face_model_batch = next(face_model_dataloader_iter)\n",
    "    except StopIteration:\n",
    "      face_model_dataloader_iter = iter(face_model_dataloader)\n",
    "      face_model_batch = next(face_model_dataloader_iter)\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    with ctx:\n",
    "      vertex_model_pred_dist = vertex_model(vertex_model_batch)\n",
    "      \n",
    "      \n",
    "      face_model_pred_dist = face_model(face_model_batch)\n",
    "      \n",
    "      \n",
    "    vertex_model_loss = -torch.sum(\n",
    "        vertex_model_pred_dist.log_prob(vertex_model_batch['vertices_flat']) * \n",
    "        vertex_model_batch['vertices_flat_mask'])  \n",
    "    \n",
    "    face_model_loss = -torch.sum(face_model_pred_dist.log_prob(face_model_batch['faces']) * \n",
    "        face_model_batch['faces_mask'])\n",
    "    \n",
    "    # Run the optimization step after sample so it uses the old parameters\n",
    "    vertex_model_optim.zero_grad()\n",
    "    vertex_model_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(vertex_model.parameters(), 1.0)\n",
    "    vertex_model_optim.step()\n",
    "\n",
    "    face_model_optim.zero_grad()\n",
    "    face_model_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(face_model.parameters(), 1.0)\n",
    "    face_model_optim.step()\n",
    "\n",
    "    # time forward pass\n",
    "    dt = time.time() - t\n",
    "    if n % log_step == 0:\n",
    "        print('Step {}'.format(n))\n",
    "        print('Loss (vertices) {}'.format(vertex_model_loss))\n",
    "        print('Loss (faces) {}'.format(face_model_loss)) \n",
    "        print('Time (ms): {}'.format(dt * 1000))\n",
    "\n",
    "        if n % sample_step == 0:\n",
    "          with ctx:\n",
    "            vertex_samples = vertex_model.sample(\n",
    "              n_samples, context=vertex_model_batch, max_sample_length=200, top_p=0.95,\n",
    "              recenter_verts=False, only_return_complete=False)\n",
    "            \n",
    "            face_samples = face_model.sample(\n",
    "              context=vertex_samples, max_sample_length=500, top_p=0.95,\n",
    "              only_return_complete=False)\n",
    "          \n",
    "          mesh_list = []\n",
    "          for n in range(min(n_samples, batch_size)):\n",
    "              mesh_list.append(\n",
    "                {\n",
    "                  'vertices': vertex_samples['vertices'][n][:vertex_samples['num_vertices'][n]].cpu(),\n",
    "                  'faces': data_utils.unflatten_faces(\n",
    "                    face_samples['faces'][n][:face_samples['num_face_indices'][n]].cpu())\n",
    "                }\n",
    "              )\n",
    "          try:\n",
    "            # sometimes theres an error when training on cpu... not sure why\n",
    "            data_utils.plot_meshes(mesh_list, ax_lims=0.5)\n",
    "          except:\n",
    "            print(\"Error plotting meshes... skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model_dataloader = DataLoader(vertex_model_dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=pad_batch)\n",
    "vertex_model_dataloader_iter = iter(vertex_model_dataloader)\n",
    "vertex_model_batch = next(vertex_model_dataloader_iter)\n",
    "\n",
    "for key, value in vertex_model_batch.items():\n",
    "    vertex_model_batch[key] = value.to('cuda')\n",
    "\n",
    "vertex_samples = vertex_model.to('cuda').sample(\n",
    "    n_samples, context=vertex_model_batch, max_sample_length=200, top_p=0.95,\n",
    "    recenter_verts=False, only_return_complete=False)    \n",
    "\n",
    "face_samples = face_model.to('cuda').sample(\n",
    "    context=vertex_samples, max_sample_length=500, top_p=0.95,\n",
    "    only_return_complete=False)\n",
    "\n",
    "mesh_list = []\n",
    "for n in range(min(n_samples, batch_size)):\n",
    "    mesh_list.append(\n",
    "    {\n",
    "        'vertices': vertex_samples['vertices'][n][:vertex_samples['num_vertices'][n]].to('cpu'),\n",
    "        'faces': data_utils.unflatten_faces(\n",
    "        face_samples['faces'][n][:face_samples['num_face_indices'][n]].to('cpu'))\n",
    "    }\n",
    "    )\n",
    "    \n",
    "data_utils.plot_meshes(mesh_list, ax_lims=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
